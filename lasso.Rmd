# understanding how lasso regression works

This is a simple example of how the lasso regression model works.

```{r run_preliminary_commands}
save.image("backup.RData")
rm(list=ls())
library("glmnet")
```

The lasso regression model was originally developed in 1989.
It is an alterative to the classic least squares estimate 
that avoids many of the problems with overfitting when
you have a large number of indepednent variables. 

You can't understand the lasso fully without understanding some
of the context of other regression models. The examples will look, 
by necessity, at a case with only two independent variables, even
though the lasso only makes sense for settings where the number of
independent variables are larger by several orders of magnitutde.

Start with a built in data set with the R package, 
[mtcars](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html).
You can get details about this data set by typing help(“mtcars”).

```{r review_of_mtcars_data}
head(mtcars)
summary(mtcars)
```

You can pick any three variables in the data set. The first variable (v0)
will be the dependent variable and the other two variables (v1, v2) will 
be the independent variables.

```{r start_with_bivariate_case}
v0 <- "mpg"
v1 <- "disp"
v2 <- "hp"
```

Examine the correlations among these variables.

```{r calculate_correlations}
round(cor(mtcars[, c(v0,v1,v2)]), 2)
```

The units of measurement for these two independent variables are quite
different, so to make comparisons more fair, you should standardize them.
There’s less of a need to standardize the dependent variable, but let’s
do that also, just for simplicity.

```{r standardize_everything}
standardize <- function(x) {(x-mean(x))/sd(x)}
z0 <- standardize(mtcars[, v0])
z1 <- standardize(mtcars[, v1])
z2 <- standardize(mtcars[, v2])
lstsq <- lm(z0~z1+z2-1)
lstsq_beta <- coef(lstsq)
```

The lm function uses the traditional linear regression approach.

Just a technical point here: the linear regression model fit here
does not include an intercept, because all the variables have 
already been standardized to a mean of zero. The simple least
squares estimates are `r round(lstsq_beta[1], 2)` for `r v1` and
`r round(lstsq_beta[2], 2)` for `r v2`.

The traditional linear regression model is sometimes called
least squares regression, because it minimizes (least)
the sum of squared deviations (squares) of the residuals.

TO understand this better, compute the residual sum of
squared deviations (rss) for a range of possible regression
coefficients.

```{r draw_minimization_surface, fig.width=5, fig.height=5}
n_lstsq <- 41
s <- seq(-1, 1, length=n_lstsq)
rss_lstsq <- matrix(NA, nrow=n_lstsq, ncol=n_lstsq)
for (i in 1:n_lstsq) {
  for (j in 1:n_lstsq) {
    rss_lstsq[i, j] <- sum((z0-s[i]*z1-s[j]*z2)^2)
  }
}
persp(s, s, rss_lstsq, xlab="beta1", ylab="beta2", zlab="rss_lstsq")
```

You may find the contour plot of this three dimensional surface to
be easier to follow.

```{r draw_contour_plot,fig.width=5, fig.height=5}
draw_axes <- function() {
  k2 <- seq(-1, 1, length=5)
  par(mar=c(4.6, 4.6, 0.6, 0.6), xaxs="i", yaxs="i")
  plot(1.02*range(s), 1.02*range(s), type="n", xlab="beta1", ylab="beta2", axes=FALSE)
  axis(side=1, pos=0, col="gray", at=k2, labels=rep(" ", length(k2)))
  axis(side=2, pos=0, col="gray", at=k2, labels=rep(" ", length(k2)))
  text(k2[-3], -0.05, k2[-3], cex=0.5, col="black")
  text(-0.05, k2[-3], k2[-3], cex=0.5, col="black")
}
k1 <- c(1, 1.1, 1.2, 1.5, 2, 2.5, 3:9)
k1 <- c(0.1*k1, k1, 10*k1, 100*k1, 1000*k1)

draw_axes()
contour(s, s, matrix(rss_lstsq,nrow=n_lstsq), levels=k1, add=TRUE, col="black")
text(lstsq_beta[1], lstsq_beta[2], "X", cex=0.5)
k1 <- k1[k1>min(rss_lstsq)]
```

The level curves (the values where the thre dimension surface is constant)
are elliptical, which reflects the correlation in $\hat\beta_1$ and $\hat\beta_2$ that is
induced by the correlation between z1 and z2.

The small "X" represents the minimum value, or the least squares solution.
It corresponds to height of `r round(min(rss_lstsq), 2)` units.

Now suppose that you were willing to sacrifice a bit on the residual sum
of squares. You'd be willing to settle for a value of $\hat\beta_1$ and $\hat\beta_2$ that
produced a residual sum of squares of `r k1[1]` instead of
`r round(min(rss_lstsq), 1)`. In exchange, you'd get a solution that was a bit
closer to zero. What would that value be? Any value on the ellipse labelled 
`r k1[1]` would be equally desirable from the least squares perspective. But
the point on the ellipse closest to (0, 0) has the most simplicity.

```{r display_ridge_solutions, fig.width=5, fig.height=5}
find_closest <- function(x, target) {
  d <- abs(x-target)
  return(which(d==min(d))[1])
}
draw_circle <- function(r) {
  radians <- seq(0, 2*pi, length=100)
  lines(r*sin(radians), r*cos(radians))
}
ridge <- glmnet(cbind(z1, z2), z0, alpha=0, intercept=FALSE, nlambda=1000)
m_ridge <- dim(ridge$beta)[2]
rss_ridge <- rep(NA,m_ridge)
for (i in 1:m_ridge) {
  rss_ridge[i] <- sum((z0 - ridge$beta[1, i]*z1 -ridge$beta[2, i]*z2)^2)
}
r1 <- find_closest(rss_ridge, k1[1])
draw_axes()
contour(s, s, matrix(rss_lstsq, nrow=n_lstsq), levels=k1, add=TRUE, col="gray")
contour(s, s, matrix(rss_lstsq, nrow=n_lstsq), levels=k1[1], add=TRUE, col="black")
draw_circle(sqrt(ridge$beta[1, r1]^2+ridge$beta[2, r1]^2))
text(lstsq_beta[1], lstsq_beta[2], "X", cex=0.5)
arrows(lstsq_beta[1], lstsq_beta[2], ridge$beta[1, r1], ridge$beta[2, r1], len=0.05)
```

For ridge regression, find the circle which just barely touches the 
ellipse corresponding to a level surface of `r k1[1]`. These values
are $\hat\beta_1$=`r round(ridge$beta[1, r1], 2)` and
$\hat\beta_2$=`r round(ridge$beta[2, r1], 2)`.

Now what do I mean by "simplicity"? In this case, I mean less of a
tendency to produce extreme predictions. Regression coefficients that
are flatter, that is, closer to zero, have less of a tendency to
produce extreme predictions.

Extreme predictions are sometimes okay, but they are often a symptom
of overfitting. 

Now ridge regression offers you a multitude of choices, depending
on the trade-offs you are willing to make between efficiency (small
rss) and simplicity (regression coefficients close to zero).

If you wanted a bit more simplicity and could suffer a bit more
on the residual sums of squares end of things, you could find
the point on the level surface `r k1[2]` or `r k1[3]`.

```{r draw_two_more, fig.width=5, fig.height=5}
r2 <- find_closest(rss_ridge, k1[2])
r3 <- find_closest(rss_ridge, k1[3])
draw_axes()
contour(s, s, matrix(rss_lstsq,nrow=n_lstsq), levels=k1, add=TRUE, col="gray")
contour(s, s, matrix(rss_lstsq,nrow=n_lstsq), levels=k1[2], add=TRUE, col="black")
draw_circle(sqrt(ridge$beta[1, r2]^2+ridge$beta[2, r2]^2))
text(lstsq_beta[1], lstsq_beta[2], "X", cex=0.5)
arrows(lstsq_beta[1], lstsq_beta[2], ridge$beta[1, r1], ridge$beta[2, r1], len=0.05)
arrows(ridge$beta[1, r1], ridge$beta[2, r1], ridge$beta[1, r2], ridge$beta[2, r2], len=0.05)

draw_axes()
contour(s, s, matrix(rss_lstsq,nrow=n_lstsq), levels=k1, add=TRUE, col="gray")
contour(s, s, matrix(rss_lstsq,nrow=n_lstsq), levels=k1[3], add=TRUE, col="black")
draw_circle(sqrt(ridge$beta[1, r3]^2+ridge$beta[2, r3]^2))
text(lstsq_beta[1], lstsq_beta[2], "X", cex=0.5)
arrows(lstsq_beta[1], lstsq_beta[2], ridge$beta[1, r1], ridge$beta[2, r1], len=0.05)
arrows(ridge$beta[1, r1], ridge$beta[2, r1], ridge$beta[1, r2], ridge$beta[2, r2], len=0.05)
arrows(ridge$beta[1, r2], ridge$beta[2, r2], ridge$beta[1, r3], ridge$beta[2, r3], len=0.05)
```

You could do this for any level surface, including those level surfaces
in between the ones shown here.

```{r draw_all_ridge_solutions, fig.width=5, fig.height=5}
draw_axes()
contour(s, s, matrix(rss_lstsq,nrow=n_lstsq), levels=k1, add=TRUE, col="gray")
text(lstsq_beta[1], lstsq_beta[2], "X", cex=0.5)
segments(lstsq_beta[1], lstsq_beta[2], ridge$beta[1, m_ridge], ridge$beta[2, m_ridge])
lines(ridge$beta[1, ], ridge$beta[2, ])
arrows(ridge$beta[1, 2], ridge$beta[2, 2], ridge$beta[1, 1], ridge$beta[2, 1], len=0.05)
```

The black curve represents all the ridge regression solutions.

Ridge regression measures simplicity as the straight line distance 
between (0, 0) and ($\hat{\beta_1}$, $\hat{\beta_2}$). This is known as Euclidean distance
or $L^2$ distance. The formula for $L^2$ distance is
$\sqrt{\hat{\beta_1} ^2+\hat{\beta_2}^2}$

Values that are at the same $L^2$ distance from the origin
correspond to circles.

The lasso regression model works much like ridge regression, except
it use $L^1$ or absolute value distance. The formula for $L^1$ distance is 
$|\hat\beta_1|+|\hat\beta_2|$. 

While $L^2$ distance represents the length of a diagonal path from 
($\hat{\beta_1}$, $\hat{\beta_2}$) to (0, 0), the $L^1$ represents the length of
the path that goes vertically to the X-axis and the horizontally to the Y-axis.

```{r L1_distance, fig.width=5, fig.height=5}
draw_axes()
arrows(-0.5, -0.5, 0, 0, len=0.05)
text(-0.25, -0.20, "L2 distance", srt=45)
arrows(-0.5, -0.5, -0.5, 0, len=0.05)
arrows(-0.5, 0, 0, 0, len=0.05)
text(-0.55, -0.01, "L1", srt=90, adj=1)
text(-0.49, 0.05, "distance", adj=0)
```

You get the same $L^1$ distance, of course, if you go first horizontally to the
Y-axis and then vertically to the X-axis. A diamond (actually a square rotated
to 45 degrees) represents the set of points that are equally distant
from the origin using the $L^1$ concept of distance.

The lasso model finds the largest diamond that just barely touches the 
level surface.

```{r draw_first_lasso, fig.width=5, fig.height=5}
lasso <- glmnet(cbind(z1, z2), z0, alpha=1, intercept=FALSE, nlambda=1000)
m_lasso <- dim(lasso$beta)[2]
rss_lasso <- rep(NA,m_lasso)
for (i in 1:m_lasso) {
  rss_lasso[i] <- sum((z0 - lasso$beta[1, i]*z1 -lasso$beta[2, i]*z2)^2)
}
r1 <- find_closest(rss_lasso, k1[1])
r2 <- find_closest(rss_lasso, k1[2])
r3 <- find_closest(rss_lasso, k1[3])

draw_axes()
contour(s, s, matrix(rss_lstsq,nrow=n_lstsq), levels=k1, add=TRUE, col="gray")
contour(s, s, matrix(rss_lstsq,nrow=n_lstsq), levels=k1[1], add=TRUE, col="black")
d <- abs(lasso$beta[1, r1])+abs(lasso$beta[2, r1])
segments( d, 0, 0, d)
segments( 0, d,-d, 0)
segments(-d, 0, 0,-d)
segments( 0,-d, d, 0)
text(lstsq_beta[1], lstsq_beta[2], "X", cex=0.5)
arrows(lstsq_beta[1], lstsq_beta[2], lasso$beta[1, r1], lasso$beta[2, r1], len=0.05)
```

Here are a couple of different lasso fits.

```{r draw_two_more_lassos, fig.width=5, fig.height=5}
draw_axes()
contour(s, s, matrix(rss_lstsq,nrow=n_lstsq), levels=k1, add=TRUE, col="gray")
contour(s, s, matrix(rss_lstsq,nrow=n_lstsq), levels=k1[2], add=TRUE, col="black")
d <- abs(lasso$beta[1, r2])+abs(lasso$beta[2, r2])
segments( d, 0, 0, d)
segments( 0, d,-d, 0)
segments(-d, 0, 0,-d)
segments( 0,-d, d, 0)
text(lstsq_beta[1], lstsq_beta[2], "X", cex=0.5)
arrows(lstsq_beta[1], lstsq_beta[2], lasso$beta[1, r1], lasso$beta[2, r1], len=0.05)
arrows(lasso$beta[1, r1], lasso$beta[2, r1], lasso$beta[1, r2], lasso$beta[2, r2], len=0.05)

draw_axes()
contour(s, s, matrix(rss_lstsq,nrow=n_lstsq), levels=k1, add=TRUE, col="gray")
contour(s, s, matrix(rss_lstsq,nrow=n_lstsq), levels=k1[3], add=TRUE, col="black")
d <- abs(lasso$beta[1, r3])+abs(lasso$beta[2, r3])
segments( d, 0, 0, d)
segments( 0, d,-d, 0)
segments(-d, 0, 0,-d)
segments( 0,-d, d, 0)
text(lstsq_beta[1], lstsq_beta[2], "X", cex=0.5)
arrows(lstsq_beta[1], lstsq_beta[2], lasso$beta[1, r1], lasso$beta[2, r1], len=0.05)
arrows(lasso$beta[1, r1], lasso$beta[2, r1], lasso$beta[1, r2], lasso$beta[2, r2], len=0.05)
arrows(lasso$beta[1, r2], lasso$beta[2, r2], lasso$beta[1, r3], lasso$beta[2, r3], len=0.05)
```

Here is the set of all lasso fits.

```{r draw_all_lasso_solutions, fig.width=5, fig.height=5}
draw_axes()
contour(s, s, matrix(rss_lstsq,nrow=n_lstsq), levels=k1, add=TRUE, col="gray")
text(lstsq_beta[1], lstsq_beta[2], "X", cex=0.5)
segments(lstsq_beta[1], lstsq_beta[2], lasso$beta[1, m_lasso], lasso$beta[2, m_lasso])
lines(lasso$beta[1, ], lasso$beta[2, ])
arrows(lasso$beta[1, 2], lasso$beta[2, 2], lasso$beta[1, 1], lasso$beta[2, 1], len=0.05)
```

While both ridge regression and lasso regression find solutions that are
closer to the origin, the lasso regression, at least in this simple case,
heads off a 45 degree angle. Once the lasso regression meets one of the
axes, it heads along that axis towards zero.

While the behavior is more complicated with more than two independent variables,
the general tendency of the lasso regression model is to shrink towards the
various axes, zeroing out one coefficient after another.

This is a bonus of lasso; It avoids overfitting not just by flattening out all
the slopes. It also avoids overfitting by preferentially removes some of the
slopes entirely.

This makes the lasso regression model a feature selection technique, and it
avoids all of the problems of overfitting that a simpler feature selection
technique, stepwise regression, suffers from.

The lasso acronym (least absolute shrinkage and selection operator) is an
description of the dual nature of this model.

Let's see how the lasso works for a slightly more complex setting. 
The mtcars data set has ten variables. Let's use the last nine
of these variables to predict the first variable, mpg.

The lasso is really intended for much bigger cases than this, but you can
still learn a lot from this small example.

```{r lasso_with_more_variables, fig.width=5, fig.height=5}
# By default, the glmnet function standardizes all the independent variables,
# but I also wanted to standardize the dependent variable for consistency
# with the earlier examples.
lasso <- glmnet(as.matrix(mtcars[, -1]), standardize(mtcars[, 1]), alpha=1, intercept=FALSE, nlambda=1000)
library("glmnet")
plot(lasso)
```

The plot above shows the shrinkage of the lasso coefficients
as you move from the right to the left, but unfortunately,
it is not clearly labelled. Here is a series of plots 
with better labels showing how the lasso coefficients change.

```{r lasso_with_labels, fig.width=5, fig.height=5}
n <- dim(lasso$beta)[1]
l1_dist <- apply(lasso$beta, 2, function(x) sum(abs(x)))
d1 <- 0.02*l1_dist[m_lasso]
d2 <- 0.18*l1_dist[m_lasso]
for (l in 12:1) {
  fraction <- l/12
  j <- max(which(l1_dist <= fraction*l1_dist[m_lasso]))
  xl <- paste("L1 distance: ", round(l1_dist[j], 2))
  plot(lasso, xlim=c(0, 1.4*max(l1_dist)),type="n", xlab=xl)
  offset <- strwidth("-", units="user")
  for (i in 1:n) {
    lines(l1_dist[1:m_lasso], lasso$beta[i, 1:m_lasso], col="lightgray")
    lines(l1_dist[1:j], lasso$beta[i, 1:j])
    if (abs(lasso$beta[i,j]) > 0) {
      text(d1+l1_dist[j], lasso$beta[i, j], names(mtcars[i+1]), adj=0)
      text(d2+l1_dist[j]+(lasso$beta[i, j]>0)*offset, lasso$beta[i, j], signif(lasso$beta[i, j], 2), adj=0)
    }
  }
}
```

Contrast this with ridge regression, which flattens out everything, but doesn't zero out any of the 
regression coefficients.

```{r ridge_with_labels, fig.width=5, fig.height=5}
ridge <- glmnet(as.matrix(mtcars[, -1]), standardize(mtcars[, 1]), alpha=0, intercept=FALSE, nlambda=1000)
m_ridge <- dim(ridge$beta)[2]
l2_dist <- apply(ridge$beta, 2, function(x) sqrt(sum(x^2)))
d1 <- 0.02*l2_dist[m_ridge]
d2 <- 0.18*l2_dist[m_ridge]
for (l in 12:1) {
  fraction <- l/12
  j <- max(which(l2_dist <= fraction*l2_dist[m_ridge]))
  xl <- paste("L2 distance: ", round(l2_dist[j], 2))
  plot(ridge, xlim=c(0, 1.4*max(l2_dist)),type="n", xlab=xl)
  offset <- strwidth("-", units="user")
  for (i in 1:n) {
    lines(l2_dist[1:m_ridge], ridge$beta[i, 1:m_ridge], col="lightgray")
    lines(l2_dist[1:j], ridge$beta[i, 1:j])
    if (abs(ridge$beta[i,j]) > 0) {
      text(d1+l2_dist[j], ridge$beta[i, j], names(mtcars[i+1]), adj=0)
      text(d2+l2_dist[j]+(ridge$beta[i, j]>0)*offset, ridge$beta[i, j], signif(ridge$beta[i, j], 2), adj=0)
    }
  }
}
```

You can also compare this to stepwise regression. Let's look at backwards elimination, because it is the easiest to program.

```{r backwards-elimination}
library("broom")
library("dplyr")
library("magrittr")
library("RColorBrewer")
mtcars.s <- data.frame(sapply(mtcars, standardize))
co <- data.frame(co=brewer.pal(10, "Set3"), term=names(mtcars.s)[-1], stringsAsFactors=FALSE)
iv.list <- names(mtcars.s)[-1]
m0 <- NULL
i <- 0
while (i < 10) {
  iv <- paste(iv.list, collapse=" + ")
  dv <- names(mtcars.s)[1]
  fo <- as.formula(paste(dv, " ~ ", iv, " - 1", sep=""))
  lm(fo, data=mtcars.s)          %>% 
    tidy                         %>%
    full_join(co)                %>%
    arrange(desc(p.value))       %>%
    mutate(step=length(iv.list))  -> m1
  m1 %>%
    slice(1)                        %>%
    mutate(abs.t=abs(statistic))    %>%
    select(step, abs.t)             %>%
    inner_join(m1)                   -> m2
  m0 %<>% bind_rows(m2)
  iv.list %<>% setdiff(m1$term[1])
  print(iv.list)
  i <- i+1
}
m0$estimate[is.na(m0$estimate)] <- 0
m0
tint <- seq(min(m0$abs.t), max(m0$abs.t), by=0.01)
abs.t <- rep(tint[1], length(tint))
for (tseq  in sort(unique(m0$abs.t))) {
  abs.t[tint > tseq] <- tseq
}

data.frame(tint=tint, abs.t=abs.t) %>%
  inner_join(m0) -> m3

for (j in seq(0, 4, by=0.5)) {
  plot(m3$tint, m3$estimate, type="n", xlim=c(4, -0.8), ylim=c(-0.9, 0.9),
       xlab=paste("|t| >", j), ylab=" ")
  for (i in unique(m0$term)) {
    sb <- which(m3$term==i)
    lines(m3$tint[sb], m3$estimate[sb], type="s", col="gray")
    sb <- which(m3$term==i & m3$tint > j)
    lines(m3$tint[sb], m3$estimate[sb], type="s", col="black")
    if (m3$estimate[sb[1]] != 0) {
      text(j-0.4, m3$estimate[sb[1]], round(m3$estimate[sb[1]],2), adj=0)
      text(j-0.05, m3$estimate[sb[1]], m3$term[sb[1]], adj=0)
    }
  }
  sb <- which(m3$tint > j & abs(m3$estimate) > 0)
}
```


You can get more complicated than this. Although the lasso regression model does fairly well,
it can sometimes get in a bit of trouble if there are several variables which are highly
intercorrelated and which all predict the dependent variable with about the same
strength. You might find better performance with Elastic Net regression, a hybrid model
that combines some of the features of ridge regression with lasso regression.

Save everything for possible re-use.

```{r save_everything}
save.image("lasso.RData")
load("backup.RData")
```

